{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rushikeshhalkanche/-Netflix-movie-and-TV-show-Clustering/blob/main/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix movie and TV show Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -*Rushikesh Halkanche*\n",
        "##### **Team Member 2 -*Prajwal Andure*\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this project is to analyze the Netflix Dataset of movies and TV shows until 2019, sourced from the third-party search engine Flixable. The goal is to group the content into relevant clusters using NLP techniques to improve the user experience through a recommendation system. This will help prevent subscriber churn for Netflix, which currently has over 220 million subscribers.\n",
        "\n",
        "Additionally, the dataset will be analyzed to uncover insights and trends in the streaming entertainment industry.\n",
        "\n",
        "The project followed a step-by-step process:\n",
        "\n",
        "Handling null values in the dataset.\n",
        "\n",
        "Managing nested columns (director, cast, listed_in, country) for better visualization.\n",
        "\n",
        "Binning the rating attribute into categories (adult, children's, family-friendly, not rated).\n",
        "\n",
        "Performing Exploratory Data Analysis (EDA) to gain insights for preventing subscriber churn.\n",
        "\n",
        "Creating clusters using attributes like director, cast, country, genre, rating, and description. These attributes were tokenized, preprocessed, and vectorized using TF-IDF vectorizer.\n",
        "\n",
        "Reducing the dimensionality of the dataset using PCA to improve performance.\n",
        "\n",
        "Employing K-Means Clustering and Agglomerative Hierarchical Clustering algorithms, determining optimal cluster numbers (4 for K-Means, 2 for hierarchical clustering) through various evaluation methods.\n",
        "\n",
        "Developing a content-based recommender system using cosine similarity matrix to provide personalized recommendations to users and reduce subscriber churn for Netflix. \\\n",
        "This comprehensive analysis and recommendation system are expected to enhance user satisfaction, leading to improved retention rates for Netflix"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rushikesh Halkanche :- https://github.com/Rushikeshhalkanche\n",
        "\n",
        "Prajwal Andure :- https://github.com/prajwalan01"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming serviceâ€™s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "## Data Maipulation Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "## Data Visualisation Libraray\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "# libraries used to process textual data\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# libraries used to implement clusters\n",
        "from sklearn.metrics import silhouette_score\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "# Library of warnings would assist in ignoring warnings issued\n",
        "import warnings;warnings.filterwarnings('ignore')\n",
        "import warnings;warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "HFILmV1-TgHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "VIwyDrFuT268"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns\n",
        "print(f\"Rows and Column count in the Dataset: Rows= {df.shape[0]}, Columns= {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"The total number of duplicated observations in the dataset: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values Percentage\n",
        "round(df.isna().sum()/len(df)*100, 2)"
      ],
      "metadata": {
        "id": "Eu3WKTwbU91e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "msno.bar(df, color='green',sort='ascending', figsize=(10,3), fontsize=15)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values using Heatmap\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.heatmap(df.isna(), cmap = 'coolwarm')"
      ],
      "metadata": {
        "id": "Bn1_YxZ5VQsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given dataset is from the online streaming industry; our task is to examine the dataset, build the clustering methods and content based recommendation system.\n",
        "\n",
        "Clustering is a technique used in machine learning and data mining to group similar data points together. A clustering algorithm is a method or technique used to identify clusters within a dataset. These clusters represent natural groupings of the data, and the goal of clustering is to discover these groupings without any prior knowledge of the groupings.\n",
        "\n",
        "There are 7787 rows and 12 columns in the dataset. In the director, cast, country, date_added, and rating columns, there are missing values. The dataset does not contain any duplicate values.\n",
        "\n",
        "Every row of information we have relates to a specific movie. Therefore, we are unable to use any method to impute any null values. Additionally, due to the small size of the data, we do not want to lose any data, so after analyzing each column, we simply impute numeric values using an empty string in the following procedure."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(f\"Available columns:\\n{df.columns.to_list()}\")"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable description of the Netflix Movies and TV Shows Clustering Dataset is as follows:\n",
        "\n",
        "1.show_id: Unique identifier for each movie/show.\n",
        "\n",
        "2.type: Indicates whether the entry is a movie or a TV show.\n",
        "\n",
        "3.title: Name of the movie or TV show.\n",
        "\n",
        "4.director: Name of the director(s) of the movie or TV show.\n",
        "\n",
        "5.cast: Names of the actors and actresses featured in the movie or TV show.\n",
        "\n",
        "6.country: Country or countries where the movie or TV show was produced.\n",
        "\n",
        "7.date_added: Date when the movie or TV show was added to Netflix.\n",
        "\n",
        "8.release_year: Year when the movie or TV show was released.\n",
        "\n",
        "9.rating: TV rating or movie rating of the movie or TV show.\n",
        "\n",
        "10.duration: Length of the movie or TV show in minutes or seasons.\n",
        "\n",
        "11.listed_in: Categories or genres of the movie or TV show.\n",
        "\n",
        "12.description: Brief synopsis or summary of the movie or TV show."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(f\"The number of unique values in: \")\n",
        "print(\"-\"*35)\n",
        "for i in df.columns:\n",
        "  print(f\"'{i}' : {df[i].nunique()}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "print(\"-\"*50)\n",
        "print(\"Null value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Let's find out the percentage of null values in each category in order to deal with it.\n",
        "print(\"Percentage of null values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"{null_count_by_variable*100}%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"date_added\"].value_counts()"
      ],
      "metadata": {
        "id": "YMIZVetJWepL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['rating'].value_counts()"
      ],
      "metadata": {
        "id": "2cmEuGbPWgzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['country'].value_counts()"
      ],
      "metadata": {
        "id": "K-UNdYqrWndH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Imputing null value as per our discussion\n",
        "# imputing with unknown in null values of director and cast feature\n",
        "df[['director','cast']]=df[['director','cast']].fillna(\"Unknown\")\n",
        "\n",
        "# Imputing null values of country with Mode\n",
        "df['country']=df['country'].fillna(df['country'].mode()[0])\n",
        "\n",
        "# Dropping remaining null values of date_added and rating\n",
        "df.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "47bs5DW8WtOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking the Missing Values/Null Values Count\n",
        "print(\"-\"*50)\n",
        "print(\"Null value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Rechecking the percentage of null values in each category\n",
        "print(\"Percentage of null values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"{null_count_by_variable*100}%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "-JDHherhW0TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a copy of dataframe and unnest the original one\n",
        "df_new= df.copy()"
      ],
      "metadata": {
        "id": "OcOq6VFsW4UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'Directors' column\n",
        "dir_constraint=df['director'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df1 = pd.DataFrame(dir_constraint, index = df['title'])\n",
        "df1 = df1.stack()\n",
        "df1 = pd.DataFrame(df1.reset_index())\n",
        "df1.rename(columns={0:'Directors'},inplace=True)\n",
        "df1 = df1.drop(['level_1'],axis=1)\n",
        "df1.sample(10)"
      ],
      "metadata": {
        "id": "Wc8RZF2tW-xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'cast' column\n",
        "cast_constraint=df['cast'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df2 = pd.DataFrame(cast_constraint, index = df['title'])\n",
        "df2 = df2.stack()\n",
        "df2 = pd.DataFrame(df2.reset_index())\n",
        "df2.rename(columns={0:'Actors'},inplace=True)\n",
        "df2 = df2.drop(['level_1'],axis=1)\n",
        "df2.sample(10)"
      ],
      "metadata": {
        "id": "DMVGjYVyXD0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'listed_in' column\n",
        "listed_constraint=df['listed_in'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df3 = pd.DataFrame(listed_constraint, index = df['title'])\n",
        "df3 = df3.stack()\n",
        "df3 = pd.DataFrame(df3.reset_index())\n",
        "df3.rename(columns={0:'Genre'},inplace=True)\n",
        "df3 = df3.drop(['level_1'],axis=1)\n",
        "df3.sample(10)"
      ],
      "metadata": {
        "id": "y8yOlMhTXJ_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'country' column\n",
        "country_constraint=df['country'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df4 = pd.DataFrame(country_constraint, index = df['title'])\n",
        "df4 = df4.stack()\n",
        "df4 = pd.DataFrame(df4.reset_index())\n",
        "df4.rename(columns={0:'Country'},inplace=True)\n",
        "df4 = df4.drop(['level_1'],axis=1)\n",
        "df4.sample(10)"
      ],
      "metadata": {
        "id": "VRipuBcZXaKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Merging all the unnested dataframes\n",
        "# Merging director and cast\n",
        "df5 = df2.merge(df1,on=['title'],how='inner')\n",
        "\n",
        "# Merging listed_in with merged of (director and cast)\n",
        "df6 = df5.merge(df3,on=['title'],how='inner')\n",
        "\n",
        "# Merging country with merged of [listed_in with merged of (director and cast)]\n",
        "df7 = df6.merge(df4,on=['title'],how='inner')\n",
        "\n",
        "# Head of final merged dataframe\n",
        "df7.head()"
      ],
      "metadata": {
        "id": "ZHBx9vjFXffW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging unnested data with the created dataframe in order to make the final dataframe\n",
        "df = df7.merge(df[['type', 'title', 'date_added', 'release_year', 'rating', 'duration','description']],on=['title'],how='left')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "46jDdw7kXmQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking info of the dataset before typecasting\n",
        "df.info()"
      ],
      "metadata": {
        "id": "93f9P5EvXsEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Typecasting duration into integer by removing 'min' and 'season' from the end\n",
        "df['duration']= df['duration'].apply(lambda x: int(x.split()[0]))\n",
        "\n",
        "# Typecasting string object to datetime object of date_added column\n",
        "df['date_added']= pd.to_datetime(df['date_added'])\n",
        "\n",
        "# Extracting date, day, month and year from date_added column\n",
        "df[\"day_added\"]= df[\"date_added\"].dt.day\n",
        "df[\"month_added\"]= df[\"date_added\"].dt.month\n",
        "df[\"year_added\"]= df[\"date_added\"].dt.year\n",
        "\n",
        "# Dropping date_added\n",
        "df.drop('date_added', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "cmWT4pWjXxsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking info of the dataset after typecasting\n",
        "df.info()"
      ],
      "metadata": {
        "id": "-0VKGFyjX4Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning the values in the rating column\n",
        "rating_map = {'TV-MA':'Adult Content',\n",
        "              'R':'Adult Content',\n",
        "              'PG-13':'Teen Content',\n",
        "              'TV-14':'Teen Content',\n",
        "              'TV-PG':'Children Content',\n",
        "              'NR':'Not Rated',\n",
        "              'TV-G':'Children Content',\n",
        "              'TV-Y':'Family-friendly Content',\n",
        "              'TV-Y7':'Family-friendly Content',\n",
        "              'PG':'Children Content',\n",
        "              'G':'Children Content',\n",
        "              'NC-17':'Adult Content',\n",
        "              'TV-Y7-FV':'Family-friendly Content',\n",
        "              'UR':'Not Rated'}\n",
        "\n",
        "df['rating'].replace(rating_map, inplace = True)\n",
        "df['rating'].unique()"
      ],
      "metadata": {
        "id": "desXJ1tcX875"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking head after binning\n",
        "df.head()"
      ],
      "metadata": {
        "id": "R9i8nINwYFm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spearating the dataframes for further analysis\n",
        "df_movies= df[df['type']== 'Movie']\n",
        "df_tvshows= df[df['type']== 'TV Show']\n",
        "\n",
        "# Printing the shape\n",
        "print(df_movies.shape, df_tvshows.shape)"
      ],
      "metadata": {
        "id": "EeAQhVKzYRN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we have imputed/drop the null values of:\n",
        "\n",
        "Imputed 'director' and 'cast' with 'Unknown'.\n",
        "\n",
        "Imputed 'country' with Mode.\n",
        "\n",
        "Drop null values of 'date_added' and 'rating' (less percentage).\n",
        "\n",
        "We have unnested values from following features:\n",
        "\n",
        "'director'\n",
        "\n",
        "'cast'\n",
        "\n",
        "'listed_in'\n",
        "\n",
        "'country'\n",
        "\n",
        "We have unnested the values and stored in different dataframes and then merged all the dataframe with the original one using left join in order to get the isolated values of each of the feature.\n",
        "\n",
        " We have typecasted the following features:\n",
        "   'duration' into integer (Removing min and seasons from the values).\n",
        "  'date_added' to datetime (Into the required format).\n",
        "\n",
        "We have also extracted the following features:\n",
        "\n",
        "'date' from 'date_added'.\n",
        "\n",
        "'month' from 'date_added'.\n",
        "\n",
        "'year' from 'date_added'."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# number of values of different categories in 'type'\n",
        "df['type'].value_counts()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,2, figsize=(14,5))\n",
        "\n",
        "# countplot\n",
        "graph = sns.countplot(x = 'type', data = df, ax=ax[0])\n",
        "graph.set_title('Count of Values', size=20)\n",
        "\n",
        "# piechart\n",
        "df['type'].value_counts().plot(kind='pie', autopct='%1.2f%%', ax=ax[1], figsize=(15,6),startangle=90)\n",
        "plt.title('Percentage Distribution', size=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PBD4LLRLZsLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us the percent of TV shows and movie data present on Netflix Data set."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the majority of the content on Netflix is movies, which account for around two-thirds of the total content. TV shows make up the remaining one-third of the content.\n",
        "\n",
        "we can conclude that in the given Data set only 28.3% are TV Shows and 71.7% are Movies"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes! the production house should more focus on quality movies because there is high competition in the market.\n",
        "\n",
        "TV Shows are less in numbers hence good opportunity for business."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "for i,j,k in ((df, 'Overall',0),(df_movies, 'Movies',1),(df_tvshows, 'TV Shows',2)):\n",
        "  plt.subplot(1,3,k+1)\n",
        "  count= i['rating'].value_counts()\n",
        "  plt.pie(count, labels=count.index,explode=(0,0,0,0,0.5),colors=['orangered','dodgerblue','lightgreen','mediumslateblue','yellow'],\n",
        "          autopct='%1.1f%%', labeldistance=1.1,wedgeprops={\"edgecolor\" : \"black\",'linewidth': 1,'antialiased': True})\n",
        "  plt.title(f\"Distribution of Content Rating on Netflix '{j}'\")\n",
        "  plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chosen this chart to know the percentage of type of content present in the Netflix."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.We found that most of the content present in the Netflix belongs to Adult and the teen categories.\n",
        "\n",
        "2.Another important insight we can see that Family friendly content less in Movies compared to TV Shows."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.For high gains production house should more focus on Teen and Adult content.\n",
        "\n",
        "2.There is good chances of growth in Family-friendly category in TV Shows"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Top 10 casts in Movies and TV Shows\n",
        "plt.style.use('default')\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_actor = i.groupby(['Actors']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[1:10]\n",
        "  plots= sns.barplot(y = \"Actors\",x = 'title', data = df_actor, palette='Set1')\n",
        "  plt.title(f'Actors appeared in most of the {j}')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which actors are more popular on Netflix."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found an interesting insight that most of the Actors in Movies are from INDIA.\n",
        "\n",
        "No popular actors from india in TV Shows."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indians are movie lover, they love to watch movies hence business should target indian audience for Movies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "df_country = df.groupby(['Country']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "plt.figure(figsize=(15,6))\n",
        "plots= sns.barplot(y = \"Country\",x = 'title', data = df_country)\n",
        "plt.xticks(rotation = 60)\n",
        "plt.title('Top 10 Countries for content creation')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plots.bar_label(plots.containers[0])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which country produces Maximum number of TV Shows and Movies."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The United States is the top country producing both movies and TV shows on Netflix. This suggests that Netflix is heavily influenced by American content.\n",
        "\n",
        "India is the second-highest producer of movies on Netflix, indicating the growing popularity of Bollywood movies worldwide."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained can have a positive impact on Netflix's business by highlighting opportunities for growth and expansion, such as investing in American and Bollywood content, and acquiring more diverse content."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Top 10 Directors in Movies and TV Shows\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_director = i.groupby(['Directors']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[1:10]\n",
        "  plots= sns.barplot(y = \"Directors\",x = 'title', data = df_director, palette='Paired')\n",
        "  plt.title(f'Directors appeared in most of the {j}')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which director is popular in Movies and which one is popular in TV Shows."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that most of the movies directed by jan suter.\n",
        "\n",
        "Most TV shows directed by ken burns."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movie/tvshows producers can select the popular director for their upcoming projects."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "#Analysing top15 countries with most content\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "sns.countplot(x=df['Country'],order=df['Country'].value_counts().index[0:15],hue=df['type'],palette =\"Set1\")\n",
        "plt.xticks(rotation=50)\n",
        "plt.title('Top 15 countries with most contents', fontsize=15, fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_country = i.groupby(['Country']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "  plots= sns.barplot(y = \"Country\",x = 'title', data = df_country, palette='Set1')\n",
        "  plt.title(f'Top 10 countries launching {j} back to back')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which country produces which type of content the most."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "INDIA Produces most amount of Movies in compare to TV Shows.\n",
        "\n",
        "Japan and South korea produces more TV Shows in compare to Movies."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained can have a positive impact on Netflix's business by highlighting opportunities for growth and expansion, such as acquiring and producing more movies from India and more TV shows from Japan and South Korea."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(23,8))\n",
        "df_genre = df.groupby(['Genre']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "plots= sns.barplot(y = \"Genre\",x = 'title', data = df_genre)\n",
        "plt.title(f'Most popular genre on Netflix')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plots.bar_label(plots.containers[0])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_genre = i.groupby(['Genre']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "  plots= sns.barplot(y = \"Genre\",x = 'title', data = df_genre, palette='Set1')\n",
        "  plt.title(f'Most popular genre of the {j}')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "  plt.yticks(rotation = 45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph tells us which genre is most popular in Netflix."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "International movies genre is most popular in both the TV Shows and Movies category. Followed by Drama and comedy."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained can have a positive impact on Netflix's business by helping the platform understand what genres and types of content are popular with its audience. This information can help Netflix tailor its content acquisition and production strategies to better cater to the preferences of its viewers, which can lead to increased engagement and customer satisfaction.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_month = i.groupby(['month_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['month_added'],ascending=False)\n",
        "  plots= sns.barplot(x = 'month_added',y='title', data = df_month, palette='husl')\n",
        "  plt.title(f'{j} added added to Netflix by month')\n",
        "  plt.ylabel(f\"Number of {j} added on Netflix\")\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  for bar in plots.patches:\n",
        "     plots.annotate(bar.get_height(),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height()), ha='center', va='center',\n",
        "                    size=12, xytext=(0, 8),\n",
        "                    textcoords='offset points')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have plotted this graph to know in which month the movie/tv shows added is maximum and in which year minimum."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that October, November and December are the most popular months for TV shows addition.\n",
        "\n",
        "January, October and December are the most popular months for movie addition"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained can help Netflix create a positive business impact by identifying the most popular months for new content additions. This can help Netflix plan content releases during peak periods, leading to increased user engagement and retention."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_day = i.groupby(['day_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['day_added'],ascending=False)\n",
        "  plots= sns.barplot(x = 'day_added',y='title', data = df_day, palette='husl')\n",
        "  plt.title(f'{j} added added to Netflix by day')\n",
        "  plt.ylabel(f\"Number of {j} added on Netflix\")\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  for bar in plots.patches:\n",
        "     plots.annotate(bar.get_height(),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height()), ha='center', va='bottom',\n",
        "                    size=12, xytext=(0, 8),\n",
        "                    textcoords='offset points', rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us the day when most of the movies added in a month."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above bar plots, it can be observed that most of the movies and TV shows are added at the beginning or middle of the month. It could be because most people tend to have more free time at the beginning of the month after getting paid, and releasing new content during that time could increase viewership. By releasing new content at the beginning and middle of the month, subscribers are more likely to feel that they are getting value for their money, which could lead to increased retention rates."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, releasing new content at regular intervals helps to keep users engaged with the platform, as they will have something new to look forward to every few weeks. This can lead to increased viewing hours and user satisfaction, both of which can have positive impacts on the business."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(20,8))\n",
        "df_year_month = df.groupby(['year_added','month_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['year_added'],ascending=False)\n",
        "sns.lineplot(x = 'year_added',y='title', data = df_year_month, palette = 'hls', hue=df_year_month['month_added'], marker='o')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bivariate graph helps us in knowing which month is dominating in adding movie/tvshows in a year."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there is no specific trend is followed, Instead of this some consecutive years shows month wise trend.\n",
        "\n",
        "From 2008 to 2009 we see movies added in the month of February, and from 2009 to 2011 movies added in the month of February and October.\n",
        "\n",
        "After 2015 majority content added in the month of october to december"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Producers should add there movies in the month when audience is more responsive.\n",
        "\n",
        "Although no specific trend is shown but most movies should be uploaded in year end with some discount in the subscription."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement 1:\n",
        "\n",
        "Null Hypothesis: There is no significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "Alternative Hypothesis: There is a significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "Hypothetical Statement 2:\n",
        "\n",
        "Null Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "Alternative Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. State Your research hypothesis as a null hypothesis and alternate hypothesis.\n",
        "Null Hypothesis: There is no significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "Alternative Hypothesis: There is a significant difference in the proportion ratings of drama movies and comedy movies on Netflix."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from statsmodels.stats.proportion import proportions_ztest  #------> This function is used to perform z test of proportion.\n",
        "\n",
        "# Subset the data to only include drama and comedy movies\n",
        "subset = df[df['Genre'].str.contains('Dramas') | df['Genre'].str.contains('Comedies')]\n",
        "\n",
        "# Calculate the proportion of drama and comedy movies\n",
        "drama_prop = len(subset[subset['Genre'].str.contains('Dramas')]) / len(subset)\n",
        "comedy_prop = len(subset[subset['Genre'].str.contains('Comedies')]) / len(subset)\n",
        "\n",
        "# Set up the parameters for the z-test\n",
        "count = [int(drama_prop * len(subset)), int(comedy_prop * len(subset))]\n",
        "nobs = [len(subset), len(subset)]\n",
        "alternative = 'two-sided'\n",
        "\n",
        "# Perform the z-test\n",
        "z_stat, p_value = proportions_ztest(count=count, nobs=nobs, alternative=alternative)\n",
        "print('z-statistic: ', z_stat)\n",
        "print('p-value: ', p_value)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results of the z-test\n",
        "if p_value < alpha:\n",
        "    print(f\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test we have used to obtain the P-value is the z-test for proportions."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The z-test for proportions was chosen because we are comparing the proportions of two categorical variables (drama movies and comedy movies) in a sample. The null hypothesis and alternative hypothesis are about the difference in proportions, and we want to determine if the observed difference in proportions is statistically significant or not. The z-test for proportions is appropriate for this situation because it allows us to compare two proportions and calculate the probability of observing the difference we see in our sample if the null hypothesis were true."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "Alternative Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# To test this hypothesis, we perform a two-sample t-test.\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Create separate dataframes for TV shows in 2020 and 2021\n",
        "tv_2020 = df[(df['type'] == 'TV Show') & (df['release_year'] == 2020)]\n",
        "tv_2021 = df[(df['type'] == 'TV Show') & (df['release_year'] == 2021)]\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t, p = ttest_ind(tv_2020['duration'].astype(int),\n",
        "                 tv_2021['duration'].astype(int), equal_var=False)\n",
        "print('t-value: ', t)\n",
        "print('p-value: ', p)\n",
        "\n",
        "# Print the results\n",
        "if p < 0.05:\n",
        "    print('Reject null hypothesis. \\nThe average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021.')\n",
        "else:\n",
        "    print('Failed to reject null hypothesis. \\nThe average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.')\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used to obtain the P-Value is a two-sample t-test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test was chosen because we are comparing the means of two different samples (TV shows added in 2020 vs TV shows added in 2021) to determine whether they are significantly different. Additionally, we assume that the two samples have unequal variances since it is unlikely that the duration of TV shows added in 2020 and 2021 would have the exact same variance."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Since we have already dealed with null value. So it is not needed now.\n",
        "df.isna().sum()\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Storing the continous value feature in a separate list\n",
        "continous_value_feature= [\"release_year\",\"duration\",\"day_added\",\"month_added\",\"year_added\"]\n",
        "\n",
        "# checking outliers with the help of box plot for continous features\n",
        "plt.figure(figsize=(16,5))\n",
        "for n,column in enumerate(continous_value_feature):\n",
        "  plt.subplot(1, 5, n+1)\n",
        "  sns.boxplot(df[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are taking the copied dataframe as the data having more number of observations resulted in ram exhaustion.\n",
        "df.shape, df_new.shape"
      ],
      "metadata": {
        "id": "gWMOgEl7zsW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning of rating in new dataframe\n",
        "df_new['rating'].replace(rating_map, inplace = True)\n",
        "\n",
        "# Checking sample after binning\n",
        "df_new.sample(2)"
      ],
      "metadata": {
        "id": "e1WIIAX6zv0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Textual Columns"
      ],
      "metadata": {
        "id": "k5uqNnYGz2vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new feature content_detail with the help of other textual attributes\n",
        "df_new[\"content_detail\"]= df_new[\"cast\"]+\" \"+df_new[\"director\"]+\" \"+df_new[\"listed_in\"]+\" \"+df_new[\"type\"]+\" \"+df_new[\"rating\"]+\" \"+df_new[\"country\"]+\" \"+df_new[\"description\"]\n",
        "\n",
        "#checking the manipulation\n",
        "df_new.head(5)"
      ],
      "metadata": {
        "id": "bjzxEEzWz_L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df_new['content_detail']= df_new['content_detail'].str.lower()\n",
        "\n",
        "# Checking the manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuations(text):\n",
        "    '''This function is used to remove the punctuations from the given sentence'''\n",
        "    #imorting needed library\n",
        "    import string\n",
        "    # replacing the punctuations with no space, which in effect deletes the punctuation marks.\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped off punctuation marks\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Punctuations from the content_detail\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_punctuations)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "YFEM9HIO0Upj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_url_and_numbers(text):\n",
        "    '''This function is used to remove the URL's and Numbers from the given sentence'''\n",
        "    # importing needed libraries\n",
        "    import re\n",
        "    import string\n",
        "\n",
        "    # Replacing the URL's with no space\n",
        "    url_number_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text= re.sub(url_number_pattern,'', text)\n",
        "\n",
        "    # Replacing the digits with one space\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # return the text stripped off URL's and Numbers\n",
        "    return text"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_url_and_numbers)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "dVp8EFop0h3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# create a set of English stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# displaying stopwords\n",
        "print(stop_words)\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_stopwords_and_whitespaces(text):\n",
        "    '''This function is used for removing the stopwords from the given sentence'''\n",
        "    text = [word for word in text.split() if not word in stopwords.words('english')]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    text=  \" \".join(text)\n",
        "\n",
        "    # removing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # return the manipulated string\n",
        "    return text"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_stopwords_and_whitespaces)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "HPIe6QuB08oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['content_detail'][0]"
      ],
      "metadata": {
        "id": "nZNCd4um1Nj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Downloading needed libraries\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization\n",
        "df_new['content_detail']= df_new['content_detail'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Importing WordNetLemmatizer from nltk module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Creating instance for wordnet\n",
        "wordnet  = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizing_sentence(text):\n",
        "    '''This function is used for lemmatizing (changing the given word into meaningfull word) the words from the given sentence'''\n",
        "    text = [wordnet.lemmatize(word) for word in text]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    text=  \" \".join(text)\n",
        "\n",
        "    # return the manipulated string\n",
        "    return text"
      ],
      "metadata": {
        "id": "Hbco_m8S1ss2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading needed libraries\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Rephrasing text by applying defined lemmatizing function\n",
        "df_new['content_detail']= df_new['content_detail'].apply(lemmatizing_sentence)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']\n"
      ],
      "metadata": {
        "id": "G8gaWnJs1ygD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Lemmatization instead of Stemming for our project because:\n",
        "\n",
        "Lemmatization produces a more accurate base word: Unlike Stemming, which simply removes the suffix from a word, Lemmatization looks at the meaning of the word and its context to produce a more accurate base form.\n",
        "\n",
        "Lemmatization can handle different inflections: Lemmatization can handle various inflections of a word, including plural forms, verb tenses, and comparative forms, making it useful for natural language processing.\n",
        "\n",
        "Lemmatization produces real words: Lemmatization always produces a real word that can be found in a dictionary, making it easier to interpret the results of text analysis.\n",
        "\n",
        "Lemmatization improves text understanding: By reducing words to their base form, Lemmatization makes it easier to understand the context and meaning of a sentence.\n",
        "\n",
        "Lemmatization supports multiple languages: While Stemming may only work well for English, Lemmatization is effective for many different languages, making it a more versatile text processing technique."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "# tokenize the text into words before POS Taging\n",
        "df_new['pos_tags'] = df_new['content_detail'].apply(nltk.word_tokenize).apply(nltk.pos_tag)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.head(5)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Importing needed libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Creating instance\n",
        "tfidfv = TfidfVectorizer(max_features=30000)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting on TfidfVectorizer\n",
        "x= tfidfv.fit_transform(df_new['content_detail'])\n",
        "\n",
        "# Checking shape of the formed document matrix\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "FyRWWfqS2iJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used TFIDF vectorization in place of BAG OF WORDS because Tf-idf vectorization takes into account the importance of each word in a document. TF-IDF also assigns higher values to rare words that are unique to a particular document, making them more important in the representation."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**4**. **Dimesionality Reduction**"
      ],
      "metadata": {
        "id": "G30wpRpG_Uoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IieKPk0p_mPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction\n",
        "# Importing PCA from sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Defining PCA object with desired number of components\n",
        "pca = PCA()\n",
        "\n",
        "# Fitting the PCA model\n",
        "pca.fit(x.toarray())\n",
        "\n",
        "# percent of variance captured by each component\n",
        "variance = pca.explained_variance_ratio_\n",
        "print(f\"Explained variance: {variance}\")"
      ],
      "metadata": {
        "id": "Le9TJZTkAFsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploting the percent of variance captured versus the number of components in order to determine the reduced dimensions\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(1, len(variance)+1), np.cumsum(pca.explained_variance_ratio_))\n",
        "ax.set_xlabel('Number of Components')\n",
        "ax.set_ylabel('Percent of Variance Captured')\n",
        "ax.set_title('PCA Analysis')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uw0umbq7GKt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now we are passing the argument so that we can capture 95% of variance.\n",
        "# Defining instance\n",
        "pca_tuned = PCA(n_components=0.95)\n",
        "\n",
        "# Fitting and transforming the model\n",
        "pca_tuned.fit(x.toarray())\n",
        "x_transformed = pca_tuned.transform(x.toarray())\n",
        "\n",
        "# Checking the shape of transformed matrix\n",
        "x_transformed.shape"
      ],
      "metadata": {
        "id": "Ax-UD_PxGX0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which dimensionality reduction technique have you used and why?"
      ],
      "metadata": {
        "id": "_OP2pGV9LQ5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used PCA (Principal Component Analysis) for dimensionality reduction. PCA is a widely used technique for reducing the dimensionality of high-dimensional data sets while retaining most of the information in the original data.\n",
        "\n",
        "PCA works by finding the principal components of the data, which are linear combinations of the original features that capture the maximum amount of variation in the data. By projecting the data onto these principal components, PCA can reduce the number of dimensions while retaining most of the information in the original data.\n",
        "\n",
        "PCA is a popular choice for dimensionality reduction because it is simple to implement, computationally efficient, and widely available in most data analysis software packages. Additionally, PCA has been extensively studied and has a strong theoretical foundation, making it a reliable and well-understood method."
      ],
      "metadata": {
        "id": "4c_O2ntzLS0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "gTOPmKsKLXuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ML Model - 1 (K-Means Clustering)"
      ],
      "metadata": {
        "id": "jeio1GDlL1Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Determining optimal value of K using KElbowVisualizer\n",
        "# Importing needed library\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "model = KMeans(random_state=0)\n",
        "visualizer = KElbowVisualizer(model, k=(1,16),locate_elbow=False)\n",
        "\n",
        "# Fit the data to the visualizer\n",
        "visualizer.fit(x_transformed)\n",
        "\n",
        "# Finalize and render the figure\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "vwvV_w_9L_8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Determining optimal value of K using KElbowVisualizer\n",
        "# Importing needed library\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "visualizer = KElbowVisualizer(model, k=(2,16), metric='silhouette', timings=True, locate_elbow=False)\n",
        "\n",
        "# Fit the data to the visualizer\n",
        "visualizer.fit(x_transformed)\n",
        "\n",
        "# Finalize and render the figure\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "ZRQS7UVBPuA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Computing Silhouette score for each k\n",
        "# Importing needed libraries\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Defining Range\n",
        "k_range = range(2, 7)\n",
        "for k in k_range:\n",
        "    Kmodel = KMeans(n_clusters=k)\n",
        "    labels = Kmodel.fit_predict(x_transformed)\n",
        "    score = silhouette_score(x, labels)\n",
        "    print(\"k=%d, Silhouette score=%f\" % (k, score))"
      ],
      "metadata": {
        "id": "DMER4gCIQABJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the K-means model on a dataset\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++', random_state= 0)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = kmeans.fit_predict(x_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x_transformed[label == i , 0] , x_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bIUJyJdhYa35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library to visualize clusters in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot the clusters in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r', 'g', 'b', 'y']\n",
        "for i in range(len(colors)):\n",
        "    ax.scatter(x_transformed[kmeans.labels_ == i, 2], x_transformed[kmeans.labels_ == i, 0], x_transformed[kmeans.labels_ == i, 1], c=colors[i])\n",
        "\n",
        "# Rotate the plot 30 degrees around the X axis and 45 degrees around the Z axis\n",
        "ax.view_init(elev=20, azim=-120)\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QxyDQjLWZVku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dateframe.\n",
        "df_new['kmeans_cluster'] = kmeans.labels_\n"
      ],
      "metadata": {
        "id": "K6zkW08sZjDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance ?"
      ],
      "metadata": {
        "id": "Uu_xhz-WZtDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting with defining a function that plot a wordcloud for each of the attribute in the given dataframe."
      ],
      "metadata": {
        "id": "7A2rpXnBZyLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_wordcloud(cluster_number, column_name):\n",
        "    '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "    #Importing libraries\n",
        "    from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "    # Filter the data by the specified cluster number and column name\n",
        "    df_wordcloud = df_new[['kmeans_cluster', column_name]].dropna()\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud['kmeans_cluster'] == cluster_number]\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud[column_name].str.len() > 0]\n",
        "\n",
        "    # Combine all text documents into a single string\n",
        "    text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "    # Create the word cloud\n",
        "    wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "    # Convert the wordcloud to a numpy array\n",
        "    image_array = wordcloud.to_array()\n",
        "\n",
        "    # Return the numpy array\n",
        "    return image_array\n"
      ],
      "metadata": {
        "id": "ji0zGAzgZ3p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(20, 15))\n",
        "for i in range(4):\n",
        "    for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(kmeans_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zCR5vEIWZ_iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model - 2 (Hierarchial Clustering)"
      ],
      "metadata": {
        "id": "cvcKf-vxaSvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing needed libraries\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# HIERARCHICAL CLUSTERING\n",
        "distances_linkage = linkage(x_transformed, method = 'ward', metric = 'euclidean')\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('All films/TV shows')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "\n",
        "dendrogram(distances_linkage, no_labels = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MrPafHv4afm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Computing Silhouette score for each k\n",
        "# Importing needed libraries\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Range selected from dendrogram above\n",
        "k_range = range(2, 10)\n",
        "for k in k_range:\n",
        "    model = AgglomerativeClustering(n_clusters=k)\n",
        "    labels = model.fit_predict(x_transformed)\n",
        "    score = silhouette_score(x, labels)\n",
        "    print(\"k=%d, Silhouette score=%f\" % (k, score))"
      ],
      "metadata": {
        "id": "RIEqQgnqbPoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the K-means model on a dataset\n",
        "Agmodel = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "\n",
        "#predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = Agmodel.fit_predict(x_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x_transformed[label == i , 0] , x_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Sz5DyPxbZaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library to visualize clusters in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot the clusters in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r', 'g', 'b', 'y']\n",
        "for i in range(len(colors)):\n",
        "    ax.scatter(x_transformed[Agmodel.labels_ == i, 0], x_transformed[Agmodel.labels_ == i, 1], x_transformed[Agmodel.labels_ == i, 2],c=colors[i])\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gu87LZif3zGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dateframe.\n",
        "df_new['agglomerative_cluster'] = Agmodel.labels_"
      ],
      "metadata": {
        "id": "FUIHTa2o4B8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart"
      ],
      "metadata": {
        "id": "G7lOrv0a4P1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just again define a function that plots wordcloud for different attributes using Agglomerative Clustering."
      ],
      "metadata": {
        "id": "101beSs-4SbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agglomerative_wordcloud(cluster_number, column_name):\n",
        "  '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "  #Importing libraries\n",
        "  from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "  # Filter the data by the specified cluster number and column name\n",
        "  df_wordcloud = df_new[['agglomerative_cluster', column_name]].dropna()\n",
        "  df_wordcloud = df_wordcloud[df_wordcloud['agglomerative_cluster'] == cluster_number]\n",
        "\n",
        "  # Combine all text documents into a single string\n",
        "  text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "  # Create the word cloud\n",
        "  wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "  # Return the word cloud object\n",
        "  return wordcloud"
      ],
      "metadata": {
        "id": "ymtcqhvQ4Zf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(20, 15))\n",
        "for i in range(2):\n",
        "    for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(agglomerative_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2w5WyoeK4hn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model - 3 (Building a Recommendaton System)"
      ],
      "metadata": {
        "id": "BRupStaw44HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing neede libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Create a TF-IDF vectorizer object and transform the text data\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(df_new['content_detail'])\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "def recommend_content(title, cosine_sim=cosine_sim, data=df_new):\n",
        "    # Get the index of the input title in the programme_list\n",
        "    programme_list = data['title'].to_list()\n",
        "    index = programme_list.index(title)\n",
        "\n",
        "    # Create a list of tuples containing the similarity score and index\n",
        "    # between the input title and all other programmes in the dataset\n",
        "    sim_scores = list(enumerate(cosine_sim[index]))\n",
        "\n",
        "    # Sort the list of tuples by similarity score in descending order\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]\n",
        "\n",
        "    # Get the recommended movie titles and their similarity scores\n",
        "    recommend_index = [i[0] for i in sim_scores]\n",
        "    rec_movie = data['title'].iloc[recommend_index]\n",
        "    rec_score = [round(i[1], 4) for i in sim_scores]\n",
        "\n",
        "    # Create a pandas DataFrame to display the recommendations\n",
        "    rec_table = pd.DataFrame(list(zip(rec_movie, rec_score)), columns=['Recommendation', 'Similarity_score(0-1)'])\n",
        "\n",
        "    return rec_table"
      ],
      "metadata": {
        "id": "h30IKrZv5BxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing indian movie\n",
        "recommend_content('Kal Ho Naa Ho')"
      ],
      "metadata": {
        "id": "vAF6gDsG5MDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing non indian movie\n",
        "recommend_content('Zombieland')"
      ],
      "metadata": {
        "id": "pV3zD5eh5Wjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing indian tv show\n",
        "recommend_content('Zindagi Gulzar Hai')"
      ],
      "metadata": {
        "id": "RROT-WYu5deW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing non indian tv show\n",
        "recommend_content('Vampires')"
      ],
      "metadata": {
        "id": "egb9iHXQ5khx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "aA6CgfnE5t1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have chosen Silhoutte Score over Distortion Score (also known as inertia or sum of squared distances) as evaluation metrics as it measures how well each data point in a cluster is separated from other clusters. It ranges from -1 to 1, with higher values indicating better cluster separation. A silhouette score close to 1 indicates that the data point is well-matched to its own cluster and poorly matched to neighboring clusters. A score close to 0 indicates that the data point is on or very close to the boundary between two clusters. A score close to -1 indicates that the data point is probably assigned to the wrong cluster.\n",
        "\n",
        "The advantages of using silhouette score over distortion score are:\n",
        "\n",
        "Silhouette score takes into account both the cohesion (how well data points within a cluster are similar) and separation (how well data points in different clusters are dissimilar) of the clusters, whereas distortion score only considers the compactness of each cluster.\n",
        "Silhouette score is less sensitive to the shape of the clusters, while distortion score tends to favor spherical clusters, and in our case the clusters are not completely spherical.\n",
        "Silhouette score provides more intuitive and interpretable results, as it assigns a score to each data point rather than just a single value for the entire clustering solution."
      ],
      "metadata": {
        "id": "JGIM-SIC51w5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusions drawn from EDA**\n",
        "\n",
        "Based on the exploratory data analysis (EDA) of the Netflix movies and TV shows clustering dataset, we have drawn the following conclusions:\n",
        "\n",
        "Movies make up about two-thirds of Netflix content, with TV shows comprising the remaining one-third.\n",
        "\n",
        "Adult and teen categories are prevalent on Netflix, while family-friendly content is more common in TV shows than in movies.\n",
        "\n",
        "Indian actors dominate Netflix movies, while popular Indian actors are absent from TV shows.\n",
        "\n",
        "Jan Suter is the most common movie director, and Ken Burns is the most common TV show director on Netflix.\n",
        "\n",
        "The United States is the largest producer of movies and TV shows on Netflix, followed by India. Japan and South Korea have more TV shows than movies, indicating growth potential in that area.\n",
        "\n",
        "International movies, drama, and comedy are the most popular genres on Netflix.\n",
        "\n",
        "TV show additions on Netflix have increased since 2018, while movie additions have decreased. In 2020, fewer movies were added compared to 2019, but more TV shows were added.\n",
        "\n",
        "October, November, and December are popular months for adding TV shows, while January, October, and November are popular for adding movies. February sees the least additions.\n",
        "\n",
        "Movies and TV shows are typically added at the beginning or middle of the month and are popularly added on weekends"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusions drawn from ML Model**\n",
        "\n",
        "Implemented K-Means Clustering and Agglomerative Hierarchical Clustering, to cluster the Netflix Movies TV show dataset.\n",
        "\n",
        "The optimal number of clusters we are getting from K-means is 4, whereas for Agglomerative Hierarchical Clustering the optimal number of clusters are found out to be 2.\n",
        "We chose Silhouette Score as the evaluation metric over distortion score because it provides a more intuitive and interpretable result. Also Silhouette score is less sensitive to the shape of the clusters.\n",
        "\n",
        "Built a Recommendation system that can help Netflix improve user experience and reduce subscriber churn by providing personalized recommendations to users based on their similarity scores."
      ],
      "metadata": {
        "id": "E8Tp_SQi6d43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "my-heE5W6dyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Thank You**"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}